{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "from math import cos, pi, radians, sqrt\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "import skmob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Mobile Data Processing and Visualization\n",
    "In this module, we will learn how to:\n",
    "\n",
    "* Process the data to conduct basic descriptive analysis including:  \n",
    "    * Print out variable names and plot distributional statistics for each (e.g., mean, standard deviation, frequency etc.);\n",
    "* Based on the descriptive statistics above, identify outliers and remove them;  \n",
    "* Process the data to deal with oscillation issues;  \n",
    "* Process the data to add various temporal variables (datetime operations); \n",
    "* Process the data to calculate a set of features on the data it including for example: number of observations, temporary occupancy etc;  \n",
    "* Use different algorithms to process the data to infer stays and a common set of mobility metrics (e.g., number of trips, radius of gyration, number of home-based tours etc.);  \n",
    "* Geocoding and reverse geocoding; \n",
    "\n",
    "Packages I will use in this module include:\n",
    "* `Pandas`\n",
    "* `NumPy`\n",
    "* `datetime`\n",
    "* `GeoPandas`\n",
    "* `scikit-mobility`\n",
    "\n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/private-data/data'\n",
    "\n",
    "# Read in data\n",
    "df = pd.read_csv(os.path.join(data_path, 'seattle_2000_all_obs_sampled.csv'))\n",
    "\n",
    "initial_shape = df.shape\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 6 columns. \n",
    "* The first column denotes the User ID and can be used to filter for a particular user's data. \n",
    "* The second and third columns denote the latitude and longitude. \n",
    "* The fourth column is the data precision (i.e., the radius in meters for which the provider has 95% certainty about the reported coordinates). \n",
    "* The fifth column is the timestamp in proper format, while the last column is a linearly-increasing timestamp where 0 is the first observation in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some descriptive statistics with the `describe()` function. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the distribution of the variables with the `hist()` function. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the data precision column has some outliers. These are erroneous data points that are not useful to keep (in fact, they probably lower our data quality; Guan et al., 2021). Let's remove them. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows which have orig_unc greater than 2000 meters\n",
    "df = df[df['orig_unc'] < 1000]\n",
    "\n",
    "# Count the rows we removed\n",
    "initial_shape[0] - df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['orig_unc'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom in on 0 - 200 meters\n",
    "df['orig_unc'][df['orig_unc'] < 200].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding distance and velocity between consecutive points\n",
    "* I give some helper functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    # Convert latitude and longitude to radians\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Calculate the difference between latitudes and longitudes\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    # Apply the haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    return c * r # Distance in kilometers\n",
    "\n",
    "def geodesic(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    geodesic distance; in meters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1 : float\n",
    "        Latitude of the first point.\n",
    "    lon1 : float\n",
    "        Longitude of the first point.\n",
    "    lat2 : float\n",
    "        Latitude of the second point.\n",
    "    lon2 : float\n",
    "        Longitude of the second point.\n",
    "    \"\"\"\n",
    "    \n",
    "    lat1 = radians(float(lat1))\n",
    "    lon1 = radians(float(lon1))\n",
    "    lat2 = radians(float(lat2))\n",
    "    lon2 = radians(float(lon2))\n",
    "    R = 6371  # radius of the earth in km\n",
    "    x = (lon2 - lon1) * cos( 0.5*(lat2+lat1) )\n",
    "    y = lat2 - lat1\n",
    "    d = R * sqrt( x*x + y*y ) * 1000\n",
    "    return d\n",
    "\n",
    "def addDist(data, type=haversine_np, lat='orig_lat', lon='orig_long'):\n",
    "    \"\"\"\n",
    "    Add distance column to a dataframe with latitudes and longitudes. \n",
    "    Type specifies whether to use the Haversine distance (default) or the geodesic distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas dataframe\n",
    "        Dataframe with latitudes and longitudes.\n",
    "    type : function\n",
    "        Function to calculate distance. Default is haversine_np.\n",
    "    lat : string\n",
    "        Name of the column with latitudes. Default is 'orig_lat'.\n",
    "    lon : string\n",
    "        Name of the column with longitudes. Default is 'orig_long'.\n",
    "    \"\"\"\n",
    "    print(\"Adding distance column to dataframe...\")\n",
    "    if type == haversine_np:\n",
    "        lat1, lon1 = data[lat], data[lon]\n",
    "        lat2, lon2 = lat1.shift(-1), lon1.shift(-1)\n",
    "        dist = type(lat1, lon1, lat2, lon2).fillna(0)\n",
    "        data['dist'] = dist\n",
    "    elif type == geodesic:\n",
    "        lat1, lon1 = data[lat], data[lon]\n",
    "        lat2, lon2 = lat1.shift(-1), lon1.shift(-1)\n",
    "        dist = type(lat1, lon1, lat2, lon2).miles.fillna(0)\n",
    "        data['dist'] = dist\n",
    "        \n",
    "def addVel(data, unix='unix_min', lat='orig_lat', lon='orig_long'):\n",
    "    \"\"\"\n",
    "    Add velocity column to a dataframe with latitudes and longitudes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas dataframe\n",
    "        Dataframe with latitudes and longitudes.\n",
    "    unix : string\n",
    "        Name of the column containing unix timestamps.\n",
    "    lat : string\n",
    "        Name of the column containing latitudes.\n",
    "    lon : string\n",
    "        Name of the column containing longitudes.\n",
    "    \"\"\"\n",
    "    print(\"Adding velocity column to dataframe...\")\n",
    "    if 'dist' in data.columns:\n",
    "        lat1, lon1 = data[lat], data[lon]\n",
    "        lat2, lon2 = lat1.shift(-1), lon1.shift(-1)\n",
    "        dist = data['dist'].fillna(0)\n",
    "        time_diff = (data[unix] - data[unix].shift(1)).fillna(0)\n",
    "        vel = dist / time_diff\n",
    "        vel.iloc[0] = 0\n",
    "        vel.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace infinite values with NaN\n",
    "        data['vel'] = vel\n",
    "    else:\n",
    "        print(\"Please run addDist method to calculate distances between points first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addDist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addVel(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding temporal depth to the data\n",
    "* Let's work with the datetime package to add some temporal depth to our data.\n",
    "    \n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See data type\n",
    "print(df['datetime'].dtype)\n",
    "# Let's first make sure that the data is properly in datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "print(df['datetime'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to augment our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of week\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "# Month\n",
    "df['month'] = df['datetime'].dt.month\n",
    "# Hour \n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "# Week\n",
    "df['week'] = df['datetime'].dt.week\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more complex operations with datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week of month\n",
    "df['week_of_month'] = df['datetime'].dt.day.apply(lambda x: math.ceil(x/7))\n",
    "\n",
    "# Seconds since midnight\n",
    "df['seconds_since_midnight'] = df['datetime'].dt.hour * 3600 + df['datetime'].dt.minute * 60 + df['datetime'].dt.second\n",
    "\n",
    "# Weekend or not\n",
    "df['weekend'] = df['day_of_week'].apply(lambda x: 1 if x > 4 else 0)\n",
    "\n",
    "# AM peak or not\n",
    "df['am_peak'] = df['hour'].apply(lambda x: 1 if (x >= 6 and x < 10) else 0)\n",
    "\n",
    "# PM peak or not\n",
    "df['pm_peak'] = df['hour'].apply(lambda x: 1 if (x >= 15 and x < 19) else 0)\n",
    "\n",
    "df[['week_of_month', 'seconds_since_midnight', 'weekend', 'am_peak', 'pm_peak']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making dummy variables\n",
    "* A lot of times, you may need to convert categorical variables into dummy variables in order to use them in regression models or other machine learning algorithms.\n",
    "* I like using `pd.get_dummies()`. It's easy and fast. \n",
    "\n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['day_of_week'], prefix='day_of_week').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the home location\n",
    "* The below is a helper function I wrote to identify the home location of a user.\n",
    "* It's based on the hypothesis that the most frequented location between 10pm and 6am is the home location.\n",
    "\n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate one random UID to plot\n",
    "uid = df['UID'].sample(1).iloc[0]\n",
    "df_uid = df[df['UID'] == uid].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newCoords(lat, lon, dy, dx):\n",
    "    \"\"\"\n",
    "    Calculates a new lat/lon from an old lat/lon + displacement in x and y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat : float\n",
    "        Latitude of the first point.\n",
    "    lon : float\n",
    "        Longitude of the first point.\n",
    "    dy : float\n",
    "        Displacement in y.\n",
    "    dx : float\n",
    "        Displacement in x.\n",
    "    \"\"\"\n",
    "    r = 6371\n",
    "    new_lat  = lat  + (dy*0.001 / r) * (180 / pi)\n",
    "    new_lon = lon + (dx*0.001 / r) * (180 / pi) / cos(lat * pi/180)\n",
    "    return new_lat, new_lon\n",
    "\n",
    "def homeLoc(data, m_threshold = 50, hour_col = 'hour'):\n",
    "    \"\"\"\n",
    "    Calculates the home location of a user based on the data provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        Dataframe containing the data.\n",
    "    m_threshold : float\n",
    "        Threshold for the maximum distance from the home location.\n",
    "    hour_col : str\n",
    "        The name of the column containing the hour information.\n",
    "    \"\"\"\n",
    "    home_lat = data[(data[hour_col] <= 6) | (data[hour_col] >= 22)]['orig_lat'].mode().item()\n",
    "    home_long = data[(data[hour_col] <= 6) | (data[hour_col] >= 22)]['orig_long'].mode().item()\n",
    "    \n",
    "    upper_b_lat, upper_b_long = newCoords(home_lat, home_long, m_threshold, 0)\n",
    "    right_b_lat, right_b_long = newCoords(home_lat, home_long, 0, m_threshold)\n",
    "    lower_b_lat, lower_b_long = newCoords(home_lat, home_long, -m_threshold, 0)\n",
    "    left_b_lat, left_b_long = newCoords(home_lat, home_long, 0, -m_threshold)\n",
    "    \n",
    "    home = []\n",
    "    \n",
    "    for i, j in enumerate(data['orig_lat']):\n",
    "        if (\n",
    "            (data['orig_lat'][i] > upper_b_lat) or\n",
    "            (data['orig_long'][i] > right_b_long) or\n",
    "            (data['orig_lat'][i] < lower_b_lat) or\n",
    "            (data['orig_long'][i] < left_b_long)\n",
    "        ):\n",
    "            home.append(0)\n",
    "        else:\n",
    "            home.append(1)\n",
    "    data['home'] = home\n",
    "\n",
    "    return home_lat, home_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeLoc(df_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-mobility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pre-existing packages like `scikit-mobility` to identify the home location. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmob.preprocessing import detection\n",
    "from skmob.preprocessing import clustering\n",
    "\n",
    "stay_clusters = clustering.cluster(\n",
    "        detection.stay_locations(\n",
    "        skmob.TrajDataFrame(df_uid, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "        , spatial_radius_km=0.2)\n",
    "        , cluster_radius_km=1)[['lat', 'lng','datetime','leaving_datetime','cluster']]\n",
    "\n",
    "stay_clusters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_clusters.value_counts('cluster').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out erroneous data points\n",
    "* Mobile data tends to have issues, particularly when cell reception is poor.\n",
    "    * Urban cores with highrises, tunnels, and other enclosed structures are particularly troublesome. This is referred to as the \"urban canyon\" effect.\n",
    "\n",
    "    * Moreover, when a signal dropped by WiFi is not immediately picked up by group of satellites, the data record can have gaps and/or be inaccurate. This is referred to as the \"cold start\" problem.\n",
    "    \n",
    "* For the above reasons, mobile datasets can have points that are physically unrealistic (i.e., oscillations between two locations, huge jumps, etc.) It is good practice to filter these points out in order to avoid introducing bias to your analysis.\n",
    "\n",
    "\n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_speed_kmh = 300 # km/h\n",
    "uncertainty_thres = 500 # number of meters in uncertainty above which we do not want observations\n",
    "\n",
    "# First, we must cast the dataframe into a TrajDataFrame\n",
    "tdf = skmob.TrajDataFrame(df_uid, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "\n",
    "# The next line will filter out all data points with speed > max_speed_kmh and remove \"loops\" aka oscillations in the data\n",
    "f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=True)\n",
    "\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows before filtering: {}\".format(tdf.shape[0]))\n",
    "print(\"Number of rows after filtering: {}\".format(f_tdf.shape[0]))\n",
    "\n",
    "# Remove data points with uncertainty > 500m\n",
    "fu_tdf = f_tdf[f_tdf['orig_unc'] <= uncertainty_thres]\n",
    "\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows after uncertainty filtering: {}\".format(fu_tdf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mobVisualize(data, test_start_date = None, test_end_date = None):\n",
    "    matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    f, (y1_ax, y2_ax) = plt.subplots(2, 1, constrained_layout = True)\n",
    "    if len(data.UID.unique()) == 1:\n",
    "        f.suptitle('User ID: ' + str(data.UID.unique().item()), fontsize = 10)\n",
    "    \n",
    "    y1_ax.scatter(data['datetime'], data['lat'], c='blue', s=1)\n",
    "    y1_ax.set_title('Latitude', fontsize = 10)\n",
    "    y1_ax.set_xticks([])\n",
    "    \n",
    "    y2_ax.scatter(data['datetime'], data['lng'], c='blue', s=1)\n",
    "    y2_ax.set_title('Longitude', fontsize = 10)\n",
    "    \n",
    "    try:\n",
    "        y1_ax.fill_between(data['datetime'], 0, 1, \n",
    "                            where=((data['datetime'] >= test_start_date) & (data['datetime'] <= test_end_date)), \n",
    "                            color='pink', alpha=0.5, label = 'Testing period', transform=y1_ax.get_xaxis_transform())\n",
    "        y2_ax.fill_between(data['datetime'], 0, 1, \n",
    "                            where=(data['datetime'] >= test_start_date) & (data['datetime'] <= test_end_date), \n",
    "                            color='pink', alpha=0.5, label = 'Testing period', transform=y2_ax.get_xaxis_transform())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    plt.xticks(rotation = 30, fontsize = 8)\n",
    "    plt.xlabel('Date', fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "mobVisualize(fu_tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial clustering\n",
    "* This is useful when we want to reduce the size of our data\n",
    "* `scikit-mobility` offers an algorithm to cluster points close to each other in space\n",
    "* $\\bf{WARNING}$: This results in losing a lot of data points that may be valuable for other purposes. Make sure to read the documentation of the function before applying.\n",
    "\n",
    "    ```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_radius_km = 0.3 # number of kilometers under which we want to cluster points\n",
    "\n",
    "fuc_tdf = skmob.preprocessing.compression.compress(fu_tdf, spatial_radius_km=spatial_radius_km)\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows after compression: {}\".format(fuc_tdf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobVisualize(fuc_tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Occupancy\n",
    "* This is a measure of completeness of the data. The way to calculate this is as follows\n",
    "    * Divide the total study period into a set of time intervals (e.g., 1 hour, 1 day, 1 week, etc.)\n",
    "    * Calculate the number of time intervals for which there is at least one data point\n",
    "    * Divide the number of time intervals for which there is at least one data point by the total number of time intervals.\n",
    "\n",
    "* The higher the temporal occupancy, the more complete the data is.\n",
    "\n",
    "* I give a helper function below to calculate temporal occupancy.\n",
    "\n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempOcp(data, bin_len = 5):\n",
    "        \"\"\"\n",
    "        Calculates the temporal occupancy of a given mobile data sequence.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bin_len : INT, optional\n",
    "            Number of minutes in a time interval (bin). The default is 5.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        temp_ocp : FLOAT\n",
    "            Temporal occupancy metric.\n",
    "\n",
    "        \"\"\"\n",
    "        bins = np.arange(min(data['unix_min']), max(data['unix_min'])+1, bin_len )\n",
    "        Nb = len(bins)\n",
    "        obs = list()\n",
    "        for i, j in enumerate(bins):\n",
    "            if i == Nb:\n",
    "                break\n",
    "            hi = list()\n",
    "            for k in range(bins[i], bins[i+1]):\n",
    "                condition = [k in data['unix_min'].values]\n",
    "                hi.append(any(condition))\n",
    "            if any(hi):\n",
    "                obs.append(1)\n",
    "            if i == (Nb-2):\n",
    "                break\n",
    "        obs.append(1)\n",
    "        temp_ocp = float(len(obs) / len(bins))\n",
    "        return temp_ocp\n",
    "\n",
    "print(tempOcp(fu_tdf)) # This is the temporal occupancy of the filtered but uncompressed data\n",
    "\n",
    "print(tempOcp(fuc_tdf)) # This is the temporal occupancy of the filtered and compressed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All other metrics\n",
    "* `scikit-mobility` offers a lot of other metrics that you can calculate on your data.\n",
    "* I give some helper functions below.\n",
    "\n",
    "    ```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmob.measures.individual\n",
    "\n",
    "def burstiness(series):\n",
    "    avg=series.mean()\n",
    "    std=series.std()\n",
    "    if (std+avg)==0:\n",
    "        B=np.nan\n",
    "    else:\n",
    "        B=(std-avg)/(std+avg) # if std+avg=0\n",
    "    return B\n",
    "\n",
    "def skmob_metric_calcs(df, lat='lat', long = 'lng', datetime = 'datetime'):\n",
    "    \"\"\"\n",
    "    Calculates scikit-mobility metrics for a dataframe with latitudes and longitudes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Dataframe with latitudes and longitudes.\n",
    "    method : string\n",
    "        Method used to generate the dataframe. Default is 'GP'.\n",
    "    lat : string\n",
    "        Name of the column containing latitudes. Default is 'lat'.\n",
    "    long : string\n",
    "        Name of the column containing longitudes. Default is 'lng'.\n",
    "    datetime : string\n",
    "        Name of the column containing datetimes. Default is 'datetime'.\n",
    "    \"\"\"\n",
    "     # Make into TrajDataFrame\n",
    "    tdf = skmob.TrajDataFrame(df, latitude=lat, longitude=long, datetime=datetime)\n",
    "\n",
    "    # Calculate scikit-mobility metrics, name parameters using method name\n",
    "    no_loc_pred = skmob.measures.individual._number_of_locations_individual(tdf)\n",
    "    rg_pred = skmob.measures.individual._radius_of_gyration_individual(tdf).squeeze()\n",
    "    k_rg_pred = skmob.measures.individual._k_radius_of_gyration_individual(tdf).squeeze()\n",
    "    jumps_pred = skmob.measures.individual._jump_lengths_individual(tdf).squeeze()\n",
    "    spat_burst_pred = burstiness(jumps_pred)\n",
    "    loc_freq_pred = skmob.measures.individual._location_frequency_individual(tdf, normalize=True) # matrix\n",
    "    rand_entr_pred = skmob.measures.individual._random_entropy_individual(tdf).squeeze()\n",
    "    real_entr_pred = skmob.measures.individual._real_entropy_individual(tdf).squeeze()\n",
    "    recency_pred = skmob.measures.individual._recency_rank_individual(tdf).squeeze()  # matrix\n",
    "    freq_rank_pred = skmob.measures.individual._frequency_rank_individual(tdf).squeeze() # matrix\n",
    "    uncorr_entr_pred = skmob.measures.individual._uncorrelated_entropy_individual(tdf).squeeze()\n",
    "    max_dist_pred = skmob.measures.individual._maximum_distance_individual(tdf).squeeze()\n",
    "    dist_straight_pred = skmob.measures.individual._distance_straight_line_individual(tdf).squeeze()\n",
    "    waiting_time_pred = skmob.measures.individual._waiting_times_individual(tdf).squeeze() # array\n",
    "    home_loc_pred = skmob.measures.individual._home_location_individual(tdf) # tuple\n",
    "    max_dist_home_pred = skmob.measures.individual._max_distance_from_home_individual(tdf).squeeze()\n",
    "    mob_network_pred = skmob.measures.individual._individual_mobility_network_individual(tdf) # big matrix\n",
    "    \n",
    "    setattr(tdf, f\"no_loc\", no_loc_pred)\n",
    "    setattr(tdf, f\"rg\", rg_pred)\n",
    "    setattr(tdf, f\"k_rg\", k_rg_pred)\n",
    "    setattr(tdf, f\"jumps\", jumps_pred)\n",
    "    setattr(tdf, f\"spat_burst\", spat_burst_pred)\n",
    "    setattr(tdf, f\"loc_freq\", loc_freq_pred)\n",
    "    setattr(tdf, f\"rand_entr\", rand_entr_pred)\n",
    "    setattr(tdf, f\"real_entr\", real_entr_pred)\n",
    "    setattr(tdf, f\"recency\", recency_pred)\n",
    "    setattr(tdf, f\"freq_rank\", freq_rank_pred)\n",
    "    setattr(tdf, f\"uncorr_entr\", uncorr_entr_pred)\n",
    "    setattr(tdf, f\"max_dist\", max_dist_pred)\n",
    "    setattr(tdf, f\"dist_straight\", dist_straight_pred)\n",
    "    setattr(tdf, f\"waiting_time\", waiting_time_pred)\n",
    "    setattr(tdf, f\"home_loc\", home_loc_pred)\n",
    "    setattr(tdf, f\"max_dist_home\", max_dist_home_pred)\n",
    "    setattr(tdf, f\"mob_network\", mob_network_pred)\n",
    "\n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = skmob_metric_calcs(fuc_tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovingPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import movingpandas as mpd\n",
    "\n",
    "tc = mpd.TrajectoryCollection(data = df, traj_id_col='UID', t = 'datetime', x='orig_long', y='orig_lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't go into much detail here, but movingPandas is another Python package you have the option of exploring to process your mobile data. It is a bit more complex than scikit-mobility, but it offers a lot more functionality. It is also well-maintained and has a lot of documentation.\n",
    "\n",
    "```python\n",
    "\n",
    "https://github.com/movingpandas/movingpandas/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoPandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Native Python Tricks to Improve Processing Times and Reduce Memory Usage\n",
    "\n",
    "* We will use the os module to get the current working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the !pwd command to get the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the os module to change the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/private-data/data'\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again print the current working directory to confirm that it has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og = pd.read_csv('seattle_2000_all_obs_sampled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took 1.3 seconds. Let's see what this file holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like a ~66 MB file containing 6 columns. The most common data type is a float64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Data Types\n",
    "* Different data types have different sizes. For example, a float64 takes up 8 bytes, while an int64 takes up 4 bytes.\n",
    "* The size of a data type is important because it determines how much memory is allocated to that variable.\n",
    "* The size of a data type also determines how fast a computer can read and write to that variable.\n",
    "* For example, a float64 takes up twice as much memory as an int64, so it will take twice as long to read and write to a float64 as it will to an int64.\n",
    "\n",
    "Let's see the range that each int type can hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show max and min values a datatype can hold\n",
    "print(np.iinfo('int8'))\n",
    "print(np.iinfo('int16'))\n",
    "print(np.iinfo('int32'))\n",
    "print(np.iinfo('int64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function that can reduce the size of a numeric dataframe by downcasting the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) \n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) \n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) \n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) \n",
    "            else: \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) \n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        else:\n",
    "            try:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "            except:\n",
    "                pass\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_compressed = reduce_mem_usage(data_og)\n",
    "print(data_compressed.dtypes)\n",
    "data_compressed.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We reduced the memory usage of our original dataframe by about 10%. On larger datasets, this is a significant gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecols Parameter in Pandas\n",
    "* Sometimes, you might have a rich dataset with many columns, but you might only be interested in a few of them.\n",
    "* In such cases, you can use the usecols parameter in the read_csv() function to read only the columns that you are interested in. This will save you a lot of memory and time.\n",
    "\n",
    "### Example: Household Travel Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTS = pd.read_csv('Household_Travel_Survey_Persons.csv')\n",
    "\n",
    "HTS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTS_short = pd.read_csv('Household_Travel_Survey_Persons.csv', usecols=['household_id', 'survey_year', 'final_home_puma10', \n",
    "                                                                              'age', 'hhsize', 'employment', 'education', 'telecommute_freq'])\n",
    "\n",
    "HTS_short.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Tips and Tools\n",
    "\n",
    "1. $\\textbf{Apply Vectorized functions}$\n",
    "\n",
    "* Whenever possible, use the built-in functions in pandas or numpy to do the operations.\n",
    "* If you have to use the pd.apply() function, see if that operation can be done with a builtin function. This is because, aply() and it’s cousins like iterrows etc will loop over the entire dataframe. \n",
    "* Whereas the builtin functions (like np.sum etc) are typically optimized for speed and usually runs much faster.\n",
    "\n",
    "2. $\\textbf{Use Numba}$\n",
    "* Numba allows you to speed up pure python functions by Just in Time (JIT) compiling them to native machine functions.\n",
    "* In several cases, you can see significant speed improvements just by adding a decorator @jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.jit\n",
    "def superplainfunc(x):\n",
    "    return x * (x + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it. Just add @numba.jit to your functions. You can parallelize your functions as well using @njit(parallel=True).\n",
    "\n",
    "To know more, here is a [5 min guide on Numba](https://numba.readthedocs.io/en/stable/user/5minguide.html)\n",
    "\n",
    "3. pd.eval and pd.query\n",
    "\n",
    "Your typical pandas code gets faster if you use pd.eval or df.query instead of using the corresponding dataframe methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating through a folder of files\n",
    "\n",
    "* If you have a folder of files, and you want to read all of them, you can use the glob module to get the list of files and then iterate through them.      \n",
    "* This is much faster than using os.listdir() to get the list of files and then iterating through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/private-data/data/ind_files/\"\n",
    "\n",
    "os.chdir(folderpath)\n",
    "\n",
    "cnt=0\n",
    "\n",
    "all_file_num = len(glob.glob(\"*.csv\"))\n",
    "\n",
    "output = pd.DataFrame(columns = ['timestamp', 'UID', 'Lat', 'Long', 'precision', 'adjusted_timestamp'])\n",
    "\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    curr = pd.read_csv(folderpath+file, header=None)\n",
    "\n",
    "    # Rename the unnamed columns\n",
    "    curr = curr.rename(columns={0:'timestamp',1:'UID',2:'Lat',3:'Long',4:'precision',5:'TimeZoneDiff'})\n",
    "\n",
    "    # Create a new column with the adjusted timestamp that starts from 0\n",
    "    curr['adjusted_timestamp'] = curr['timestamp'] - curr['timestamp'].min()\n",
    "\n",
    "    # Convert timestamp column to datetime\n",
    "    curr['timestamp'] = pd.to_datetime(curr['timestamp'] * 1e9)\n",
    "    \n",
    "    # Sort values by adjusted timestamp\n",
    "    curr = curr.sort_values(by=['adjusted_timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # Drop the TimeZoneDiff column\n",
    "    curr = curr.drop(columns='TimeZoneDiff')\n",
    "\n",
    "    # Append to output\n",
    "    output = pd.concat([output, curr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parallelizing DataFrame and Vector Operations on the GPU with cuDF and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import time as t\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up a large CSV file\n",
    "file_path = \"/mnt/c/Users/ekino/OneDrive - UW/big-data-tutorial/data/seattle_2000_all_obs_sampled.csv\"\n",
    "\n",
    "time_start = t.time()\n",
    "pdf = pd.read_csv(file_path)\n",
    "time_end = t.time()\n",
    "read_time_pd = time_end - time_start\n",
    "print(\"Pandas read_csv took \", read_time_pd, \" seconds.\")\n",
    "\n",
    "time_start = t.time()\n",
    "cdf = cudf.read_csv(file_path)\n",
    "time_end = t.time()\n",
    "read_time_cdf = time_end - time_start\n",
    "print(\"Cudf read_csv took \", read_time_cdf, \" seconds\")\n",
    "# What's the speed up of Pandas over CuDF?\n",
    "print(\"Pandas is \", int(round(read_time_cdf/read_time_pd)), \" times faster than CuDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by column (in descending order)\n",
    "time_start = t.time()\n",
    "pdf.sort_values(by=['datetime'], ascending=False, inplace=True)\n",
    "time_end = t.time()\n",
    "sort_time_pd = time_end - time_start\n",
    "print(\"Pandas sort took: \", sort_time_pd, \" seconds\")\n",
    "\n",
    "time_start = t.time()\n",
    "cdf = cdf.sort_values(by=['datetime'], ascending=False)\n",
    "time_end = t.time()\n",
    "sort_time_cdf = time_end - time_start\n",
    "print(\"CuDF sort took: \", sort_time_cdf, \" seconds\")\n",
    "print(\"CuDF is \", int(round(sort_time_pd/sort_time_cdf)), \" times faster than Pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['datetime'] = pd.to_datetime(pdf['datetime'])\n",
    "cdf['datetime'] = cudf.to_datetime(cdf['datetime'])\n",
    "\n",
    "start_time = t.time()\n",
    "pdf['dayofweek'] = pdf['datetime'].dt.dayofweek\n",
    "pdf['unix'] = pdf['datetime'].astype('int64')//1e9\n",
    "end_time = t.time()\n",
    "dayofweek_time_pd = end_time - start_time\n",
    "\n",
    "start_time = t.time()\n",
    "cdf['dayofweek'] = cdf['datetime'].dt.dayofweek\n",
    "cdf['unix'] = cdf['datetime'].astype('int64')//1e9\n",
    "end_time = t.time()\n",
    "dayofweek_time_cdf = end_time - start_time\n",
    "\n",
    "print(\"Pandas dayofweek took: \", dayofweek_time_pd, \" seconds\")\n",
    "print(\"CuDF dayofweek took: \", dayofweek_time_cdf, \" seconds\")\n",
    "print(\"CuDF is \", int(round(dayofweek_time_pd/dayofweek_time_cdf)), \" times faster than Pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try the $\\texttt{.apply}$ method to create a new column that specifies whether it is a weekday or weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that is 1 if the day is a weekend, 0 if not\n",
    "start_time = t.time()\n",
    "pdf['weekend'] = pdf['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "end_time = t.time()\n",
    "weekend_time_pd = end_time - start_time\n",
    "\n",
    "start_time = t.time()\n",
    "cdf['weekend'] = cdf['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "end_time = t.time()\n",
    "weekend_time_cdf = end_time - start_time\n",
    "\n",
    "print(\"Pandas weekend took: \", weekend_time_pd, \" seconds\")\n",
    "print(\"CuDF weekend took: \", weekend_time_cdf, \" seconds\")\n",
    "print(\"CuDF is \", int(round(weekend_time_pd/weekend_time_cdf)), \" times faster than Pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding up for loops\n",
    "We need to derive a function that allows us to first associate each observation with a trip or a stop, then cluster stop points by some spatial radius, and assign each observation a stop cluster ID.\n",
    "* If an observation is not associated with a stop, then it is a moving point, which we will denote by \"99\".\n",
    "* This process will allow us to identify hot spots of activity.\n",
    "\n",
    "Let's try to apply it on both Pandas and CuDF dataframes.\n",
    "\n",
    "To save time, we will work with only one user's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random user\n",
    "uid = pdf['UID'].sample(n=1).iloc[0]\n",
    "\n",
    "# Pandas\n",
    "start_time = t.time()\n",
    "user_data_pd = pdf[pdf['UID'] == uid]\n",
    "end_time = t.time()\n",
    "user_data_time_pd = end_time - start_time\n",
    "\n",
    "# CuDF\n",
    "start_time = t.time()\n",
    "user_data_cdf = cdf[cdf['UID'] == uid]\n",
    "end_time = t.time()\n",
    "user_data_time_cdf = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are a few helper functions that we will leverage in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sqrt\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees); in meters\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude to radians\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Calculate the difference between latitudes and longitudes\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    # Apply the haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    return c * r * 1000 # Output distance in meters\n",
    "\n",
    "def geodesic(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    geodesic distance; in meters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1 : float\n",
    "        Latitude of the first point.\n",
    "    lon1 : float\n",
    "        Longitude of the first point.\n",
    "    lat2 : float\n",
    "        Latitude of the second point.\n",
    "    lon2 : float\n",
    "        Longitude of the second point.\n",
    "    \"\"\"\n",
    "    \n",
    "    lat1 = radians(float(lat1))\n",
    "    lon1 = radians(float(lon1))\n",
    "    lat2 = radians(float(lat2))\n",
    "    lon2 = radians(float(lon2))\n",
    "    R = 6371  # radius of the earth in km\n",
    "    x = (lon2 - lon1) * cos( 0.5*(lat2+lat1) )\n",
    "    y = lat2 - lat1\n",
    "    d = R * sqrt( x*x + y*y ) * 1000\n",
    "    return d\n",
    "\n",
    "# This is a helper function to add a distance column to a dataframe with latitudes and longitudes.\n",
    "def addDist(data, type=haversine_np, lat='orig_lat', lon='orig_long', verbose=False):\n",
    "    \"\"\"\n",
    "    Add distance column to a dataframe with latitudes and longitudes. \n",
    "    Type specifies whether to use the Haversine distance (default) or the geodesic distance. \n",
    "    Returned distance is in meters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas dataframe\n",
    "        Dataframe with latitudes and longitudes.\n",
    "    type : function\n",
    "        Function to calculate distance. Default is haversine_np.\n",
    "    lat : string\n",
    "        Name of the column with latitudes. Default is 'orig_lat'.\n",
    "    lon : string\n",
    "        Name of the column with longitudes. Default is 'orig_long'.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Adding distance column to dataframe...\")\n",
    "    if type == haversine_np:\n",
    "        # Calculate distance between each point and the next point\n",
    "        lat1, lon1 = data[lat], data[lon]\n",
    "        # Shift lat/lon columns by 1 row to get the next point\n",
    "        lat2, lon2 = lat1.shift(-1), lon1.shift(-1)\n",
    "        dist = type(lat1, lon1, lat2, lon2).fillna(0)\n",
    "        data['dist'] = dist\n",
    "    elif type == geodesic:\n",
    "        lat1, lon1 = data[lat], data[lon]\n",
    "        lat2, lon2 = lat1.shift(-1), lon1.shift(-1)\n",
    "        dist = type(lat1, lon1, lat2, lon2).miles.fillna(0)\n",
    "        data['dist'] = dist\n",
    "\n",
    "# This is a helper function to add a velocity column to a dataframe with latitudes and longitudes.\n",
    "def addVel(data, unix='unix_min', lat='orig_lat', lon='orig_long', verbose=False):\n",
    "    \"\"\"\n",
    "    Add velocity column to a dataframe with latitudes and longitudes. \n",
    "    Speed in meters/second.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas dataframe\n",
    "        Dataframe with latitudes and longitudes.\n",
    "    unix : string\n",
    "        Name of the column containing unix timestamps.\n",
    "    lat : string\n",
    "        Name of the column containing latitudes.\n",
    "    lon : string\n",
    "        Name of the column containing longitudes.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Adding velocity column to dataframe...\")\n",
    "    if 'dist' in data.columns:\n",
    "        lat1, lon1 = data[lat], data[lon]\n",
    "        lat2, lon2 = lat1.shift(-1), lon1.shift(-1)\n",
    "        dist = data['dist'].fillna(0)\n",
    "        time_diff = (data[unix] - data[unix].shift(1)).fillna(0)\n",
    "        vel = dist / time_diff\n",
    "        vel.iloc[0] = 0\n",
    "        vel.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace infinite values with NaN\n",
    "        data['vel'] = vel\n",
    "    else:\n",
    "        print(\"Please run addDist method to calculate distances between points first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first remember what our original data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's use scikit-mobility (a Python package) to retrieve the stay clusters\n",
    "import skmob\n",
    "from skmob.preprocessing import detection\n",
    "from skmob.preprocessing import clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance columns\n",
    "addDist(user_data_pd)\n",
    "addDist(user_data_cdf)\n",
    "\n",
    "# Add velocity columns\n",
    "addVel(user_data_pd, 'unix', 'orig_lat', 'orig_long')\n",
    "addVel(user_data_cdf, 'unix', 'orig_lat', 'orig_long')\n",
    "\n",
    "# Add datetime columns\n",
    "user_data_pd['datetime'] = pd.to_datetime(user_data_pd['unix'], unit='s')\n",
    "user_data_cdf['datetime'] = cudf.to_datetime(user_data_cdf['unix'], unit='s')\n",
    "\n",
    "# Get stay clusters: We will only do this with the Pandas dataframe, since the CuDF dataframe\n",
    "# is not supported by scikit-mobility\n",
    "stay_clusters = clustering.cluster(\n",
    "        detection.stay_locations(\n",
    "        skmob.TrajDataFrame(user_data_pd, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "        , spatial_radius_km=0.2)\n",
    "        , cluster_radius_km=1)[['datetime','leaving_datetime','cluster']]\n",
    "\n",
    "stay_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_clusters['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through toy dataframe: if datetime column is between leaving_datetime and datetime, then assign a cluster value (new column) to that row\n",
    "# Initialize cluster column. If cluster = 99, it means that the point is not in a stay location cluster (i.e., it is a moving point)\n",
    "start_time = t.time()\n",
    "user_data_pd['cluster'] = 99\n",
    "for i in range(1, len(user_data_pd)):\n",
    "    for j in range(0, len(stay_clusters)):\n",
    "        if user_data_pd['datetime'].iloc[i] >= stay_clusters['datetime'][j] and user_data_pd['datetime'].iloc[i] <= stay_clusters['leaving_datetime'][j]:\n",
    "            user_data_pd['cluster'].iloc[i] = stay_clusters['cluster'][j]\n",
    "            break\n",
    "end_time = t.time()\n",
    "cluster_time_pd = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CPU vs. GPU Architectures](img/cpu_vs_gpu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_cluster(df_datetime, stay_clusters_datetime, \n",
    "                stay_clusters_leaving_datetime, stay_clusters_cluster, \n",
    "                df_cluster):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < df_datetime.shape[0] and j < stay_clusters_datetime.shape[0]:\n",
    "        if df_datetime[i] >= stay_clusters_datetime[j] and df_datetime[i] <= stay_clusters_leaving_datetime[j]:\n",
    "            df_cluster[i] = stay_clusters_cluster[j]\n",
    "\n",
    "def stayLocClusterConversion(df, stay_clusters, datetime_col='timestamp', verbose=False):\n",
    "    df_datetime = df[datetime_col].to_numpy()\n",
    "    df['cluster'] = 99\n",
    "    df_cluster = df['cluster'].to_numpy()\n",
    "    stay_clusters_datetime = stay_clusters['datetime'].to_numpy()\n",
    "    stay_clusters_leaving_datetime = stay_clusters['leaving_datetime'].to_numpy()\n",
    "    stay_clusters_cluster = stay_clusters['cluster'].to_numpy()\n",
    "\n",
    "    df_cluster = cuda.to_device(df_cluster)\n",
    "    df_datetime = cuda.to_device(df_datetime)\n",
    "    stay_clusters_datetime = cuda.to_device(stay_clusters_datetime)\n",
    "    stay_clusters_leaving_datetime = cuda.to_device(stay_clusters_leaving_datetime)\n",
    "    stay_clusters_cluster = cuda.to_device(stay_clusters_cluster)\n",
    "\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = math.ceil(df_datetime.shape[0] / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(stay_clusters_datetime.shape[0] / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "    start = cuda.event()\n",
    "    end = cuda.event()\n",
    "    start.record()\n",
    "    add_cluster[blockspergrid, threadsperblock](df_datetime, stay_clusters_datetime, stay_clusters_leaving_datetime, stay_clusters_cluster, df_cluster)\n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "    if verbose:\n",
    "        print(\"Time it took: \", cuda.event_elapsed_time(start, end), \"ms\")\n",
    "    df_cluster = df_cluster.copy_to_host()\n",
    "    df['cluster'] = df_cluster\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Serial approach took: \", cluster_time_pd, \" seconds\")\n",
    "print(\"\")\n",
    "\n",
    "# Parallelized version using numba\n",
    "start_time = t.time()\n",
    "user_data_cdf = stayLocClusterConversion(user_data_cdf, stay_clusters, datetime_col='datetime')\n",
    "end_time = t.time()\n",
    "cluster_time_cdf = end_time - start_time\n",
    "print(\"\")\n",
    "print(\"Parallelized approach took: \", cluster_time_cdf, \" seconds\")\n",
    "print(\"\")\n",
    "print(\"Parallelized approach is \", int(round(cluster_time_pd/cluster_time_cdf)), \" times faster than serial approach\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
